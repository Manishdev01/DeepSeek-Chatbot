# ğŸš€ DeepSeek-R1-1.5B Deployment Guide

Powered by Ollama

## ğŸ“¥ Installation Requirements

### Ollama Setup

| Platform | Command/Link |
|---|---|
| ğŸ§ Linux | `curl -fsSL https://ollama.com/install.sh | sh` |
| ğŸ macOS | [Download Installer](https://ollama.com/) |
| ğŸªŸ Windows | [Download Installer](https://ollama.com/) |

## ğŸ¤– Model Installation

Get the DeepSeek R1 model from:

ğŸ”— [Ollama Model Library](https://ollama.com/library)

\`\`\`bash
ollama run deepseek-r1:[TAG]  # Replace [TAG] with model version
\`\`\`

## ğŸ–¥ï¸ VPS Configuration Guide

### Model Selection Matrix

| VPS Tier | Specifications | Recommended Model |
|---|---|---|
| ğŸ‡ Low-End | 2-4 vCPU, <8GB RAM | \`deepseek-r1:1.5b\` |
| ğŸ¦Œ Medium | 4-8 vCPU, 8-16GB RAM | \`deepseek-r1:7b\` |
| ğŸ High-End | 8+ vCPU, 16+GB RAM | \`deepseek-r1:14b\` |

### ğŸ”§ Network Configuration

#### API Endpoint Setup

\`VPS_IP:11438\`

#### Firewall Rules

\`\`\`bash
sudo ufw allow 11438/tcp
sudo ufw reload
\`\`\`

For alternative setups, consider [reverse proxy configuration](https://www.google.com/url?sa=E&source=gmail&q=https://nginx.org/en/docs/http/reverse_proxy.html) (example link to nginx reverse proxy documentation).

### ğŸš¦ Service Management

\`\`\`bash
# Create systemd service
sudo systemctl enable ollama
sudo systemctl start ollama
\`\`\`

## ğŸ‰ Ready to Deploy\!

Your personal DeepSeek-R1 instance is now accessible at:

\`http://[YOUR_VPS_IP]:11438/api/generate\`

## ğŸ“Œ Pro Tip:

For production environments, consider:

* ğŸ” SSL/TLS encryption
* ğŸ”„ Load balancing
* ğŸ“Š Monitoring with \`ollama serve --verbose\`

\`\`\`diff
+ Successfully deployed!
- Remember to secure your endpoint!
\`\`\`
